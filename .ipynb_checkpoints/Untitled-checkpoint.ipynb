{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d9bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from requests import get\n",
    "from json import dump\n",
    "from os import getcwd\n",
    "from os import path\n",
    "from os import mkdir\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## FUNCTIONS\n",
    "def getRawHTML(link):\n",
    "    \n",
    "    r = get(link, headers={'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'})\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def retrieveServiceData(data):\n",
    "    \n",
    "    all = data.find_all('table', {\"class\": \"typetable\"})\n",
    "    \n",
    "    serviceArr = []\n",
    "    for item in all[0].find_all('tr'):\n",
    "        serviceObj = {}\n",
    "        \n",
    "        try:\n",
    "            service = item.find_all('td')[0].find('a', {\"class\": \"link\"}).text\n",
    "            serviceObj['service'] = service\n",
    "        except:\n",
    "            service = 'NA'\n",
    "        \n",
    "        try:\n",
    "            desc = item.find_all('td')[1].text.replace(\"\\n\", \"\")\n",
    "            serviceObj['description'] = desc\n",
    "        except:\n",
    "            desc = 'NA'\n",
    "        \n",
    "        try:\n",
    "            postStr = item.find('a', {\"class\": 'link'})['href']\n",
    "            link = base_url + postStr\n",
    "            serviceObj['link'] = link\n",
    "        except:\n",
    "            link = 'NA'\n",
    "    \n",
    "        if serviceObj:\n",
    "            serviceArr.append(serviceObj)\n",
    "    \n",
    "    fileDir = dirPath +'/service'\n",
    "    if not path.exists(fileDir):\n",
    "        mkdir(fileDir)\n",
    "        \n",
    "    with open(path.join(fileDir, '') + 'service.json', 'w+') as outfile:\n",
    "        dump(serviceArr, outfile)\n",
    "        \n",
    "    return serviceArr\n",
    "\n",
    "def retrieveOperationData(link, index):\n",
    "    \n",
    "    data = getRawHTML(link)\n",
    "    \n",
    "    all = data.find_all('table', {\"class\": \"typetable\"})\n",
    "\n",
    "    operationArr = []\n",
    "    for item in all[0].find_all('tr'):\n",
    "        operationOj = {}\n",
    "        \n",
    "        try:\n",
    "            ops = item.find_all('td')[0].find('a', {\"class\": \"link\"}).text\n",
    "            operationOj['operation'] = ops\n",
    "        except:\n",
    "            ops = 'NA'\n",
    "        \n",
    "        try:\n",
    "            desc = item.find_all('td')[1].text.replace(\"\\n\", \"\").replace(\"\\u00a0\", \"\")\n",
    "            operationOj['description'] = desc\n",
    "        except:\n",
    "            desc = 'NA'\n",
    "        \n",
    "        try:\n",
    "            postStr = item.find('a', {\"class\": 'link'})['href']\n",
    "            link = base_url + serviceArr[index]['service'] + '/v38.2/' + postStr\n",
    "            link = link.strip()\n",
    "            operationOj['link'] = link\n",
    "        except:\n",
    "            link = 'NA'\n",
    "    \n",
    "        if operationOj:\n",
    "            operationOj['service'] = serviceArr[index]['service']\n",
    "            operationArr.append(operationOj)\n",
    "    \n",
    "    fileDir = dirPath +'/operation'\n",
    "    if not path.exists(fileDir):\n",
    "        mkdir(fileDir)\n",
    "        \n",
    "    individual_ops_target_dir = fileDir + '/'+ operationOj['service'] +'/'\n",
    "    if not path.exists(individual_ops_target_dir):\n",
    "        mkdir(individual_ops_target_dir)\n",
    "    \n",
    "    with open(path.join(individual_ops_target_dir, '') + operationOj['service'] + '.json', 'w+') as outfile:\n",
    "        dump(operationArr, outfile)\n",
    "    \n",
    "    return operationArr\n",
    "\n",
    "def retrieve(hrefName):\n",
    "    fieldsArr = []\n",
    "    for i in data:\n",
    "        for j in i.find_all('td'):\n",
    "            if j.find('a', {\"name\": hrefName}):\n",
    "                fieldsData = (j.find('table', {\"class\": 'typetable'})).find_all('tr')\n",
    "                for item in fieldsData:\n",
    "                    if len(item.find_all('td')) > 2:\n",
    "                        fieldsObj = {}\n",
    "                        try:\n",
    "                            fieldsObj['parameterName'] = item.find_all('td')[0].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                        except:\n",
    "                            fieldsObj['parameterName'] = 'NA'\n",
    "\n",
    "                        try:\n",
    "                            fieldsObj['type'] = item.find_all('td')[1].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                            if ((item.find_all('td')[1]).find('a')):\n",
    "                                if ((item.find_all('td')[1]).find('a')).get('href'): \n",
    "                                    fieldsObj[item.find_all('td')[1].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")] = retrieve(((item.find_all('td')[1]).find('a')).get('href')[1:])\n",
    "                        except:\n",
    "                            fieldsObj['type'] = 'NA'\n",
    "                            \n",
    "                        try:\n",
    "                            fieldsObj['cardinality'] = item.find_all('td')[2].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                        except:\n",
    "                            fieldsObj['cardinality'] = 'NA'\n",
    "    \n",
    "                        try:\n",
    "                            fieldsObj['description'] = item.find_all('td')[3].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                        except:\n",
    "                            fieldsObj['description'] = 'NA'\n",
    "                            \n",
    "                        if fieldsObj:\n",
    "                            fieldsArr.append(fieldsObj)\n",
    "    \n",
    "    return fieldsArr\n",
    "\n",
    "def retrieveFieldData(link, service, operation):\n",
    "    \n",
    "    print(\"=====================\")\n",
    "    print(link)\n",
    "    \n",
    "    rawFieldData = getRawHTML(link)    \n",
    "    ulList = rawFieldData.find_all('ul')[2]\n",
    "    requesthreflink = ((ulList.find('li')).find('a'))['href'][1:]\n",
    "    data = rawFieldData.find_all('table')[0].find_all('tr')\n",
    "    \n",
    "    xyz = retrieve(requesthreflink)\n",
    "    \n",
    "    print(\"=====================\")\n",
    "    print(service)\n",
    "    print(operation)\n",
    "    print(fileDir)\n",
    "    print(path.join(fileDir, '') + operation + \".json\")\n",
    "    fileDir = dirPath +'/operation/' + service + '/fields'\n",
    "    if not path.exists(fileDir):\n",
    "        mkdir(fileDir)\n",
    "    with open(path.join(fileDir, '') + operation + \".json\", \"w+\") as outfile:\n",
    "        dump(arr, outfile)\n",
    "        \n",
    "    return True\n",
    "\n",
    "dirPath = getcwd()\n",
    "\n",
    "base_url = \"https://community.workday.com/sites/default/files/file-hosting/productionapi/\"\n",
    "\n",
    "## Service Extraction\n",
    "rawServiceData = getRawHTML(\"https://community.workday.com/sites/default/files/file-hosting/productionapi/index.html\")\n",
    "serviceArr = retrieveServiceData(rawServiceData)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pool1 = Pool()\n",
    "    result_async = [pool1.apply_async(retrieveOperationData, args = (serviceArr[index]['link'].strip(), index, )) for index in\n",
    "                    range(len(serviceArr))]\n",
    "    \n",
    "    result_async = [result_async[id].get() for id in range(len(result_async))]\n",
    "        \n",
    "    for id in tqdm(range(len(result_async))):\n",
    "        print(\"===========================\")\n",
    "        print(result_async[id][0]['link'].strip())\n",
    "        result_async1 = [pool1.apply_async(retrieveFieldData, args = (result_async[id][index]['link'].strip(), result_async[id][index]['service'], result_async[id][index]['operation'], )) for index in\n",
    "                    range(len(result_async[id]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = [] \n",
    "for ops in range(len(result_async)):\n",
    "    \n",
    "    result_async1 = [pool1.apply_async(getRawHTML, args = (result_async[ops][index]['link'].strip(), )) for index in\n",
    "                    range(len(result_async[ops]))]\n",
    "    \n",
    "    result_async1 = [result_async1[id].get() for id in range(len(result_async1))]\n",
    "    process.append(result_async1)\n",
    "    #result_async1 = [result_async1[id].get() for id in range(len(result_async1))]\n",
    "    \n",
    "    #print(len(result_async))\n",
    "    \n",
    "    #for index in range(len(result_async[ops])):\n",
    "        \n",
    "        #rawFieldData = getRawHTML(result_async[ops][index]['link'].strip())\n",
    "        \n",
    "        #ulList = rawFieldData.find_all('ul')[2]\n",
    "        #requesthreflink = ((ulList.find('li')).find('a'))['href'][1:]\n",
    "        #data = rawFieldData.find_all('table')[0].find_all('tr')\n",
    "        \n",
    "        #arr = retrieveFieldData(requesthreflink)\n",
    "        \n",
    "        #rawFieldData = getRawHTML(result_async[ops][index]['link'].strip())\n",
    "        #ulList = rawFieldData.find_all('ul')[2]\n",
    "        #requesthreflink = ((ulList.find('li')).find('a'))['href'][1:]\n",
    "        #data = rawFieldData.find_all('table')[0].find_all('tr')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d858f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing #importing the module\n",
    " \n",
    "def even(n): #function to print all even numbers till n\n",
    "    for i in range(0,n,2):\n",
    "        print(\"even:\",i)\n",
    "def odd(n): #function to print all odd numbers till n\n",
    "    for i in range(1,n,2):\n",
    "        print(\"odd:\",i)\n",
    "if __name__==\"__main__\":\n",
    "    # creating processes for each of the functions\n",
    "    prc1 = multiprocessing.Process(target=even, args=(15, ))\n",
    "    prc2 = multiprocessing.Process(target=odd, args=(15, ))\n",
    "    # starting the 1st process\n",
    "    prc1.start()\n",
    "    # starting the 2nd process\n",
    "    prc2.start()\n",
    "    # waiting until 1st process is finished\n",
    "    prc1.join()\n",
    "    # waiting until 2nd process is finished\n",
    "    prc2.join()\n",
    "    # both processes finished\n",
    "    print(\"END!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e858d597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Field Level Extraction: ACA_Partner_Integrations: 100%|██████████| 4/4 [00:00<00:00, 4991.73it/s]\n",
      "Field Level Extraction: Absence_Management: 100%|██████████| 14/14 [00:00<00:00, 24264.57it/s]\n",
      "Field Level Extraction: Academic_Advising: 100%|██████████| 29/29 [00:00<00:00, 43753.53it/s]\n",
      "Field Level Extraction: Academic_Foundation: 100%|██████████| 69/69 [00:00<00:00, 36596.73it/s]\n",
      "Field Level Extraction: Admissions: 100%|██████████| 40/40 [00:00<00:00, 73648.88it/s]\n",
      "Field Level Extraction: Adoption: 100%|██████████| 2/2 [00:00<00:00, 4481.09it/s]\n",
      "Field Level Extraction: Benefits_Administration: 100%|██████████| 29/29 [00:00<00:00, 65642.10it/s]\n",
      "Field Level Extraction: Campus_Engagement: 100%|██████████| 5/5 [00:00<00:00, 32214.32it/s]\n",
      "Field Level Extraction: Cash_Management: 100%|██████████| 99/99 [00:00<00:00, 45232.69it/s]\n",
      "Field Level Extraction: Compensation: 100%|██████████| 61/61 [00:00<00:00, 79779.40it/s]\n",
      "Field Level Extraction: Compensation_Review: 100%|██████████| 9/9 [00:00<00:00, 13324.65it/s]\n",
      "Field Level Extraction: Drive: 100%|██████████| 4/4 [00:00<00:00, 5029.14it/s]\n",
      "Field Level Extraction: Dynamic_Document_Generation: 100%|██████████| 4/4 [00:00<00:00, 27869.13it/s]\n",
      "Field Level Extraction: External_Integrations: 100%|██████████| 1/1 [00:00<00:00, 1415.56it/s]\n",
      "Field Level Extraction: Financial_Aid: 100%|██████████| 56/56 [00:00<00:00, 28742.17it/s]\n",
      "Field Level Extraction: Financial_Management: 100%|██████████| 333/333 [00:00<00:00, 82816.68it/s]\n",
      "Field Level Extraction: Human_Resources: 100%|██████████| 250/250 [00:00<00:00, 6420.30it/s]\n",
      "Field Level Extraction: Identity_Management: 100%|██████████| 2/2 [00:00<00:00, 5200.62it/s]\n",
      "Field Level Extraction: Integrations: 100%|██████████| 28/28 [00:00<00:00, 53822.42it/s]\n",
      "Field Level Extraction: Interview_Feedback__Do_Not_Use_: 100%|██████████| 2/2 [00:00<00:00, 15857.48it/s]\n",
      "Field Level Extraction: Inventory: 100%|██████████| 74/74 [00:00<00:00, 14906.99it/s]\n",
      "Field Level Extraction: Learning: 100%|██████████| 84/84 [00:00<00:00, 90200.09it/s]\n",
      "Field Level Extraction: Metadata_Translations: 100%|██████████| 1/1 [00:00<00:00, 9058.97it/s]\n",
      "Field Level Extraction: Moments: 100%|██████████| 14/14 [00:00<00:00, 40948.57it/s]\n",
      "Field Level Extraction: Notification: 100%|██████████| 1/1 [00:00<00:00, 2225.09it/s]\n",
      "Field Level Extraction: Org_Studio: 100%|██████████| 4/4 [00:00<00:00, 31300.78it/s]\n",
      "Field Level Extraction: Payroll: 100%|██████████| 90/90 [00:00<00:00, 96175.12it/s]\n",
      "Field Level Extraction: Payroll_CAN: 100%|██████████| 33/33 [00:00<00:00, 63144.18it/s]\n",
      "Field Level Extraction: Payroll_FRA: 100%|██████████| 7/7 [00:00<00:00, 6805.78it/s]\n",
      "Field Level Extraction: Payroll_GBR: 100%|██████████| 58/58 [00:00<00:00, 28697.61it/s]\n",
      "Field Level Extraction: Payroll_Interface: 100%|██████████| 27/27 [00:00<00:00, 46185.24it/s]\n",
      "Field Level Extraction: Performance_Management: 100%|██████████| 42/42 [00:00<00:00, 75865.96it/s]\n",
      "Field Level Extraction: Professional_Services_Automation: 100%|██████████| 2/2 [00:00<00:00, 5511.57it/s]\n",
      "Field Level Extraction: Recruiting: 100%|██████████| 90/90 [00:00<00:00, 44588.63it/s]\n",
      "Field Level Extraction: Resource_Management: 100%|██████████| 317/317 [00:00<00:00, 82317.63it/s]\n",
      "Field Level Extraction: Revenue_Management: 100%|██████████| 227/227 [00:00<00:00, 35162.94it/s]\n",
      "Field Level Extraction: Scheduling: 100%|██████████| 8/8 [00:00<00:00, 10258.16it/s]\n",
      "Field Level Extraction: Settlement_Services: 100%|██████████| 23/23 [00:00<00:00, 45959.50it/s]\n",
      "Field Level Extraction: Staffing: 100%|██████████| 113/113 [00:00<00:00, 94867.16it/s]\n",
      "Field Level Extraction: Student_Core: 100%|██████████| 32/32 [00:00<00:00, 67108.86it/s]\n",
      "Field Level Extraction: Student_Finance: 100%|██████████| 51/51 [00:00<00:00, 71541.64it/s]\n",
      "Field Level Extraction: Student_Records: 100%|██████████| 120/120 [00:00<00:00, 90248.61it/s]\n",
      "Field Level Extraction: Student_Recruiting: 100%|██████████| 33/33 [00:00<00:00, 57266.05it/s]\n",
      "Field Level Extraction: Talent: 100%|██████████| 104/104 [00:00<00:00, 90687.65it/s]\n",
      "Field Level Extraction: Tenant_Data_Translation: 100%|██████████| 8/8 [00:00<00:00, 48003.48it/s]\n",
      "Field Level Extraction: Time_Tracking: 100%|██████████| 6/6 [00:00<00:00, 9671.72it/s]\n",
      "Field Level Extraction: Workday_Connect: 100%|██████████| 10/10 [00:00<00:00, 44763.12it/s]\n",
      "Field Level Extraction: Workday_Extensibility: 100%|██████████| 2/2 [00:00<00:00, 13640.01it/s]\n",
      "Field Level Extraction: Workforce_Planning: 100%|██████████| 6/6 [00:00<00:00, 9998.34it/s]\n"
     ]
    }
   ],
   "source": [
    "## FUNCTIONS\n",
    "def getRawHTML(link):\n",
    "    \n",
    "    r = get(link, headers={'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'})\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def retrieveServiceData(data):\n",
    "    \n",
    "    all = data.find_all('table', {\"class\": \"typetable\"})\n",
    "    \n",
    "    serviceArr = []\n",
    "    for item in all[0].find_all('tr'):\n",
    "        serviceObj = {}\n",
    "        \n",
    "        try:\n",
    "            service = item.find_all('td')[0].find('a', {\"class\": \"link\"}).text\n",
    "            serviceObj['service'] = service\n",
    "        except:\n",
    "            service = 'NA'\n",
    "        \n",
    "        try:\n",
    "            desc = item.find_all('td')[1].text.replace(\"\\n\", \"\")\n",
    "            serviceObj['description'] = desc\n",
    "        except:\n",
    "            desc = 'NA'\n",
    "        \n",
    "        try:\n",
    "            postStr = item.find('a', {\"class\": 'link'})['href']\n",
    "            link = base_url + postStr\n",
    "            serviceObj['link'] = link\n",
    "        except:\n",
    "            link = 'NA'\n",
    "    \n",
    "        if serviceObj:\n",
    "            serviceArr.append(serviceObj)\n",
    "    \n",
    "    fileDir = dirPath +'/service'\n",
    "    if not path.exists(fileDir):\n",
    "        mkdir(fileDir)\n",
    "        \n",
    "    with open(path.join(fileDir, '') + 'service.json', 'w+') as outfile:\n",
    "        dump(serviceArr, outfile)\n",
    "        \n",
    "    return serviceArr\n",
    "\n",
    "def retrieveOperationData(data, index):\n",
    "    \n",
    "    all = data.find_all('table', {\"class\": \"typetable\"})\n",
    "\n",
    "    operationArr = []\n",
    "    for item in all[0].find_all('tr'):\n",
    "        operationOj = {}\n",
    "        \n",
    "        try:\n",
    "            ops = item.find_all('td')[0].find('a', {\"class\": \"link\"}).text\n",
    "            operationOj['operation'] = ops\n",
    "        except:\n",
    "            ops = 'NA'\n",
    "        \n",
    "        try:\n",
    "            desc = item.find_all('td')[1].text.replace(\"\\n\", \"\").replace(\"\\u00a0\", \"\")\n",
    "            operationOj['description'] = desc\n",
    "        except:\n",
    "            desc = 'NA'\n",
    "        \n",
    "        try:\n",
    "            postStr = item.find('a', {\"class\": 'link'})['href']\n",
    "            link = base_url + serviceArr[index]['service'] + '/v38.2/' + postStr\n",
    "            link = link.strip()\n",
    "            operationOj['link'] = link\n",
    "        except:\n",
    "            link = 'NA'\n",
    "    \n",
    "        if operationOj:\n",
    "            operationOj['service'] = serviceArr[index]['service']\n",
    "            operationArr.append(operationOj)\n",
    "    \n",
    "    fileDir = dirPath +'/operation'\n",
    "    if not path.exists(fileDir):\n",
    "        mkdir(fileDir)\n",
    "        \n",
    "    individual_ops_target_dir = fileDir + '/'+ serviceArr[index]['service'] +'/'\n",
    "    if not path.exists(individual_ops_target_dir):\n",
    "        mkdir(individual_ops_target_dir)\n",
    "    \n",
    "    with open(path.join(individual_ops_target_dir, '') + serviceArr[index]['service'] + '.json', 'w+') as outfile:\n",
    "        dump(operationArr, outfile)\n",
    "    \n",
    "    return operationArr\n",
    "\n",
    "def retrieve(hrefName, data):\n",
    "    fieldsArr = []\n",
    "    for i in data:\n",
    "        for j in i.find_all('td'):\n",
    "            if j.find('a', {\"name\": hrefName}):\n",
    "                fieldsData = (j.find('table', {\"class\": 'typetable'})).find_all('tr')\n",
    "                for item in fieldsData:\n",
    "                    if len(item.find_all('td')) > 2:\n",
    "                        fieldsObj = {}\n",
    "                        try:\n",
    "                            fieldsObj['parameterName'] = item.find_all('td')[0].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                        except:\n",
    "                            fieldsObj['parameterName'] = 'NA'\n",
    "\n",
    "                        try:\n",
    "                            fieldsObj['type'] = item.find_all('td')[1].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                            if ((item.find_all('td')[1]).find('a')):\n",
    "                                if ((item.find_all('td')[1]).find('a')).get('href'): \n",
    "                                    fieldsObj[item.find_all('td')[1].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")] = retrieve(((item.find_all('td')[1]).find('a')).get('href')[1:], data)\n",
    "                        except:\n",
    "                            fieldsObj['type'] = 'NA'\n",
    "                            \n",
    "                        try:\n",
    "                            fieldsObj['cardinality'] = item.find_all('td')[2].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                        except:\n",
    "                            fieldsObj['cardinality'] = 'NA'\n",
    "    \n",
    "                        try:\n",
    "                            fieldsObj['description'] = item.find_all('td')[3].text.replace(\"\\n\", \"\").replace(\"\\xa0\", \"\")\n",
    "                        except:\n",
    "                            fieldsObj['description'] = 'NA'\n",
    "                            \n",
    "                        if fieldsObj:\n",
    "                            fieldsArr.append(fieldsObj)\n",
    "        \n",
    "    return fieldsArr\n",
    "\n",
    "def retrieveFieldData(link, service, operation):\n",
    "    \n",
    "    rawFieldData = getRawHTML(link)    \n",
    "    ulList = rawFieldData.find_all('ul')[2]\n",
    "    requesthreflink = ((ulList.find('li')).find('a'))['href'][1:]\n",
    "    data = rawFieldData.find_all('table')[0].find_all('tr')\n",
    "    xyz = retrieve(requesthreflink, data)\n",
    "\n",
    "    if service:\n",
    "        fileDir = dirPath +'/operation/' + service + '/fields'\n",
    "        if not path.exists(fileDir):\n",
    "            mkdir(fileDir)\n",
    "        \n",
    "        if operation:\n",
    "            with open(path.join(fileDir, '') + operation + \".json\", \"w+\") as outfile:\n",
    "                dump(xyz, outfile)\n",
    "        else:\n",
    "            print(\"NOT OPERATION\")\n",
    "    else:\n",
    "        print(\"NOT SERVICE\")\n",
    "    \n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "from requests import get\n",
    "from json import dump\n",
    "from os import getcwd\n",
    "from os import path\n",
    "from os import mkdir\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool, Process\n",
    "import threading\n",
    "\n",
    "dirPath = getcwd()\n",
    "\n",
    "base_url = \"https://community.workday.com/sites/default/files/file-hosting/productionapi/\"\n",
    "\n",
    "## Service Extraction\n",
    "rawServiceData = getRawHTML(\"https://community.workday.com/sites/default/files/file-hosting/productionapi/index.html\")\n",
    "serviceArr = retrieveServiceData(rawServiceData)\n",
    "pool=Pool(processes=5)\n",
    "process = []\n",
    "threads = []\n",
    "## Operations Extraction\n",
    "for index in range(len(serviceArr)):\n",
    "    rawOperationData = getRawHTML(serviceArr[index]['link'].strip())\n",
    "    operationArr = retrieveOperationData(rawOperationData, index)\n",
    "    \n",
    "    ## Fields of Operation Extraction\n",
    "    for id in tqdm(range(len(operationArr)), desc=f\"Field Level Extraction: {serviceArr[index]['service']}\"):\n",
    "        #pool.apply_async(retrieveFieldData,(operationArr[id]['link'].strip(), operationArr[id]['service'], operationArr[id]['operation'],))\n",
    "        t = threading.Thread(target=retrieveFieldData, args=(operationArr[id]['link'].strip(), operationArr[id]['service'], operationArr[id]['operation'],))\n",
    "        threads.append(t)\n",
    "        #t.start()\n",
    "        #p=Process(target=retrieveFieldData,args=(operationArr[id]['link'].strip(), operationArr[id]['service'], operationArr[id]['operation'],))\n",
    "        #process.append(p)\n",
    "        #p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0829b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 22/657 [02:44<2:41:06, 15.22s/it]"
     ]
    }
   ],
   "source": [
    "x = range(0, len(threads), 4)\n",
    "for p in tqdm(x):\n",
    "    threads[p].start()\n",
    "    threads[p+1].start()\n",
    "    threads[p+2].start()\n",
    "    threads[p+3].start()\n",
    "    \n",
    "    threads[p].join()\n",
    "    threads[p+1].join()\n",
    "    threads[p+2].join()\n",
    "    threads[p+3].join()\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in threads:\n",
    "    print(x.isAlive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b215e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
